{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saitejakatta7/Image-Captioning-with-Attention/blob/main/Image_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twCPKl2Fp3Oc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjIZbWHGqDor"
      },
      "source": [
        "Load Images and Captions\n",
        "\n",
        "Let’s load the full dataset into a Python dictionary:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL9YlZNLqMXy",
        "outputId": "9e8df3d7-b95e-4d57-9d24-360660c27fda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzPyH8bwq7LT"
      },
      "outputs": [],
      "source": [
        "!unzip /content/gdrive/MyDrive/Flicker8k/archive.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6omHVYCLrvrc",
        "outputId": "a12b83f1-b834-424d-d37a-c53817382dd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "captions.txt  gdrive  Images  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTflUyJNrwrM",
        "outputId": "a8f576b0-8a9e-4c59-a3be-3644aaacdde5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Images\n"
          ]
        }
      ],
      "source": [
        "cd ./Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "60AuYhLrr5BI",
        "outputId": "3747f4b6-146d-4976-8b33-5a1cdae3d0b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Images'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OegyksGjr7pl"
      },
      "outputs": [],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcDzTOrbsLwz"
      },
      "outputs": [],
      "source": [
        "caption_file = \"/content/captions.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5q_Hc5sJsICP"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_captions (filename):\n",
        "  with open(filename, \"r\") as fp:\n",
        "    # Read all text in the file\n",
        "    text = fp.read()\n",
        "    return (text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RWfbScasZDX"
      },
      "outputs": [],
      "source": [
        "#--------------------------------------------------\n",
        "# Each photo has a unique identifier, which is the file name of the image .jpg file\n",
        "# Create a dictionary of photo identifiers (without the .jpg) to captions. Each photo identifier maps to\n",
        "# a list of one or more textual descriptions.\n",
        "#\n",
        "# {\"image_name_1\" : [\"caption 1\", \"caption 2\", \"caption 3\"],\n",
        "#  \"image_name_2\" : [\"caption 4\", \"caption 5\"]}\n",
        "#--------------------------------------------------\n",
        "def captions_dict(text):\n",
        "  dict = {}\n",
        "  \n",
        "  # Make a List of each line in the file\n",
        "  lines = text.split('\\n')\n",
        "  for line in lines:\n",
        "    \n",
        "    # Split into the <image_data> and <caption>\n",
        "    line_split = line.split(',')\n",
        "    if (len(line_split) != 2):\n",
        "      # Added this check because dataset contains some blank lines\n",
        "      continue\n",
        "    else:\n",
        "      image_data, caption = line_split\n",
        "      #1000268201_693b08cb0e.jpg --> image_data\n",
        "\n",
        "    # Split into <image_file> and <caption_idx>\n",
        "    #image_file, caption_idx = image_data.split('#')\n",
        "    # Split the <image_file> into <image_name>.jpg\n",
        "    image_name = image_data.split('.')[0]\n",
        "    \n",
        "    # If this is the first caption for this image, create a new list for that\n",
        "    # image and add the caption to it. Otherwise append the caption to the \n",
        "    # existing list\n",
        "    if image_name not in dict:\n",
        "      dict[image_name] = [caption]\n",
        "    else:\n",
        "      dict[image_name].append(caption)\n",
        "  \n",
        "  return (dict)\n",
        "\n",
        "doc = load_captions(caption_file)\n",
        "image_dict = captions_dict(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBqink882jC5"
      },
      "outputs": [],
      "source": [
        "doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayhYcnIs3CVb",
        "outputId": "d3a69371-a295-4b81-da43-41d43af2bbb1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['image,caption',\n",
              " '1000268201_693b08cb0e.jpg,A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
              " '1000268201_693b08cb0e.jpg,A girl going into a wooden building .',\n",
              " '1000268201_693b08cb0e.jpg,A little girl climbing into a wooden playhouse .',\n",
              " '1000268201_693b08cb0e.jpg,A little girl climbing the stairs to her playhouse .',\n",
              " '1000268201_693b08cb0e.jpg,A little girl in a pink dress going into a wooden cabin .',\n",
              " '1001773457_577c3a7d70.jpg,A black dog and a spotted dog are fighting',\n",
              " '1001773457_577c3a7d70.jpg,A black dog and a tri-colored dog playing with each other on the road .',\n",
              " '1001773457_577c3a7d70.jpg,A black dog and a white dog with brown spots are staring at each other in the street .',\n",
              " '1001773457_577c3a7d70.jpg,Two dogs of different breeds looking at each other on the road .']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "lines = doc.split('\\n')\n",
        "lines[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia9539U215j0",
        "outputId": "4bc4f714-8c30-42b7-b915-4cc7c7419606"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image,caption\n",
            "1000268201_693b08cb0e.jpg,A child in a pink dress is climbing up a set of stairs in an entry way .\n",
            "1000268201_693b08cb0e.jpg,A girl going into a wooden building .\n",
            "1000268201_693b08cb0e.jpg,A little girl climbing into a wooden playhouse .\n",
            "1000268201_693b08cb0e.jpg,A little girl climbing the stairs to her playhouse .\n",
            "1000268201_693b08cb0e.jpg,A little girl in a pink dress going into a wooden cabin .\n",
            "1001773457_577c3a7d70.jpg,A black dog and a spotted dog are fighting\n",
            "1001773457_577c3a7d70.jpg,A black dog and a tri-colored dog playing with each other on the road .\n",
            "1001773457_577c3a7d70.jpg,A black dog and a white dog with brown spots are staring at each other in the street .\n",
            "['1001773457_577c3a7d70.jpg', 'A black dog and a tri-colored dog playing with each other on the road .']\n"
          ]
        }
      ],
      "source": [
        "i=1\n",
        "for line in lines:\n",
        "    print(line)\n",
        "    #print(type(line))\n",
        "    i+=1\n",
        "    if(i==10):\n",
        "      break\n",
        "\n",
        "    # Split into the <image_data> and <caption>\n",
        "    line_split = str(line).split(',')\n",
        "print(line_split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrNAe4-H16YW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adm9xyfcsrAj"
      },
      "outputs": [],
      "source": [
        "#--------------------------------------------------\n",
        "# We have three separate files which contain the names for the subset of \n",
        "# images to be used for training, validation or testing respectively\n",
        "#\n",
        "# Given a file, we return a set of image names (without .jpg extension) in that file\n",
        "#--------------------------------------------------\n",
        "def subset_image_name (filename):\n",
        "  data = []\n",
        "  \n",
        "  with open(filename, \"r\") as fp:\n",
        "    # Read all text in the file\n",
        "    text = fp.read()\n",
        "  \n",
        "    # Make a List of each line in the file\n",
        "    lines = text.split ('\\n')\n",
        "    for line in lines:\n",
        "      # skip empty lines\n",
        "      if (len(line) < 1):\n",
        "        continue\n",
        "      \n",
        "      # Each line is the <image_file>\n",
        "      # Split the <image_file> into <image_name>.jpg\n",
        "      image_name = line.split ('.')[0]\n",
        "      \n",
        "      # Add the <image_name> to the list\n",
        "      data.append (image_name)\n",
        "\n",
        "    return (set(data))  \n",
        "\n",
        "training_image_name_file = \"/content/gdrive/MyDrive/Flicker8k/train_images.txt\"\n",
        "training_image_names = subset_image_name (training_image_name_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaJqnji8uHvU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9400c54-0544-451b-808c-b5006454b1d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 5s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "#image Preprocessing\n",
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path) #it is just as with open (filename)\n",
        "    img = tf.image.decode_jpeg(img, channels=3) #decodes a JPEG-encoded image into a tensor. \n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img) #it pre-processes an image for input into the Inception V3 model. This function takes an image tensor as input and performs scalling \n",
        "    return img, image_path\n",
        "\n",
        "\n",
        "#inception v3 model for the feature extraction\n",
        "#we are not including the fully connected layers as we are only intrested in extracting the features within the images\n",
        "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                                weights='imagenet')\n",
        "new_input = image_model.input  #an image tensor with shape (batch_size, height, width, channels)\n",
        "hidden_layer = image_model.layers[-1].output  #By calling image_model.layers[-1], we are accessing the last layer of the model, which is the output tensor of the final convolutional layer before the classification layers.\n",
        "\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer) #tf.keras.Model is a class in TensorFlow Keras that allows you to create arbitrary models by specifying the inputs and outputs of the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gU_rQ9StvABA",
        "outputId": "9d1d8515-5fe1-49a7-e8e6-cffc160e6281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 375/375 [01:37<00:00,  3.83it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "image_dir = \"/content/Images/\"\n",
        "training_image_paths = [image_dir + name + '.jpg' for name in training_image_names]\n",
        "\n",
        "# Get unique images\n",
        "encode_train = sorted(set(training_image_paths))\n",
        "\n",
        "# Feel free to change batch_size according to your system configuration\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train) #see in word doc\n",
        "image_dataset = image_dataset.map(\n",
        "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
        "#The num_parallel_calls argument specifies the number of parallel calls to use for the map operation. \n",
        "#In this case, tf.data.experimental.AUTOTUNE is used, \n",
        "#which allows TensorFlow to dynamically tune the number of parallel calls based on available system resources.\n",
        "\n",
        "for img, path in tqdm(image_dataset):\n",
        "  batch_features = image_features_extract_model(img)   #it is the inception model\n",
        "  batch_features = tf.reshape(batch_features,\n",
        "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "  for bf, p in zip(batch_features, path):\n",
        "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "    np.save(path_of_feature, bf.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcTiD8aO-X6i"
      },
      "outputs": [],
      "source": [
        "# 375 batches of 16 batch size = 6000 training images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO3FS1bWw30s"
      },
      "source": [
        "**Preparing Captions**  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re"
      ],
      "metadata": {
        "id": "E-ko_JnAyaZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXTVDQyixSih"
      },
      "outputs": [],
      "source": [
        "#--------------------------------------------------\n",
        "# Clean the captions data\n",
        "#    Convert all words to lowercase.\n",
        "#    Remove all punctuation.\n",
        "#    Remove all words that are one character or less in length (e.g. ‘a’).\n",
        "#    Remove all words with numbers in them.\n",
        "#--------------------------------------------------\n",
        "def captions_clean (image_dict):\n",
        "  # <key> is the image_name, which can be ignored\n",
        "  for key, captions in image_dict.items():\n",
        "    \n",
        "    # Loop through each caption for this image\n",
        "    for i, caption in enumerate (captions):\n",
        "      \n",
        "      # Convert the caption to lowercase, and then remove all special characters from it\n",
        "      caption_nopunct = re.sub(r\"[^a-zA-Z0-9]+\", ' ', caption.lower())\n",
        "      \n",
        "      # Split the caption into separate words, and collect all words which are more than \n",
        "      # one character and which contain only alphabets (ie. discard words with mixed alpha-numerics)\n",
        "      clean_words = [word for word in caption_nopunct.split() if ((len(word) > 1) and (word.isalpha()))]\n",
        "      \n",
        "      # Join those words into a string\n",
        "      caption_new = ' '.join(clean_words)\n",
        "      \n",
        "      # Replace the old caption in the captions list with this new cleaned caption\n",
        "      captions[i] = caption_new\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCJPkmppyXvE"
      },
      "outputs": [],
      "source": [
        "#--------------------------------------------------\n",
        "# Add two tokens, 'startseq' and 'endseq' at the beginning and end respectively, \n",
        "# of every caption\n",
        "#--------------------------------------------------\n",
        "def add_token (captions):\n",
        "  for i, caption in enumerate (captions):\n",
        "    captions[i] = 'startseq ' + caption + ' endseq'\n",
        "  return (captions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBMNAypjydwx"
      },
      "outputs": [],
      "source": [
        "#--------------------------------------------------\n",
        "# Given a set of training, validation or testing image names, return a dictionary\n",
        "# containing the corresponding subset from the full dictionary of images with captions\n",
        "#\n",
        "# This returned subset has the same structure as the full dictionary\n",
        "# {\"image_name_1\" : [\"caption 1\", \"caption 2\", \"caption 3\"],\n",
        "#  \"image_name_2\" : [\"caption 4\", \"caption 5\"]}\n",
        "#--------------------------------------------------\n",
        "def subset_data_dict (image_dict, image_names):\n",
        "  dict = { image_name:add_token(captions) for image_name,captions in image_dict.items() if image_name in image_names}\n",
        "  return (dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZEmvvlZymRc"
      },
      "outputs": [],
      "source": [
        "#--------------------------------------------------\n",
        "# Flat list of all captions\n",
        "#--------------------------------------------------\n",
        "def all_captions (data_dict):\n",
        "  return ([caption for key, captions in data_dict.items() for caption in captions])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yzg4rs65yt61"
      },
      "outputs": [],
      "source": [
        "#--------------------------------------------------\n",
        "# Calculate the word-length of the caption with the most words\n",
        "#--------------------------------------------------\n",
        "def max_caption_length(captions):\n",
        "  return max(len(caption.split()) for caption in captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gc0Q6CGs0ZI1"
      },
      "outputs": [],
      "source": [
        "\n",
        "image_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7PY1O5h1caa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thn9DAXc0pDE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-v6D8ypyzne"
      },
      "outputs": [],
      "source": [
        "#--------------------------------------------------\n",
        "# Fit a Keras tokenizer given caption descriptions\n",
        "# The tokenizer uses the captions to learn a mapping from words to numeric word indices\n",
        "#\n",
        "# Later, this tokenizer will be used to encode the captions as numbers\n",
        "#--------------------------------------------------\n",
        "def create_tokenizer(data_dict):\n",
        "  captions = all_captions(data_dict)\n",
        "  max_caption_words = max_caption_length(captions)\n",
        "  \n",
        "  # Initialise a Keras Tokenizer\n",
        "  tokenizer = Tokenizer()\n",
        "  \n",
        "  # Fit it on the captions so that it prepares a vocabulary of all words\n",
        "  tokenizer.fit_on_texts(captions)\n",
        "  \n",
        "  # Get the size of the vocabulary\n",
        "  vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "  return (tokenizer, vocab_size, max_caption_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsM7Z9rYy3mj"
      },
      "outputs": [],
      "source": [
        "#--------------------------------------------------\n",
        "# Extend a list of text indices to a given fixed length\n",
        "#--------------------------------------------------\n",
        "def pad_text (text, max_length): \n",
        "  text = pad_sequences([text], maxlen=max_length, padding='post')[0]\n",
        "  \n",
        "  return (text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BGg6HoVzAgC"
      },
      "outputs": [],
      "source": [
        "captions_clean(image_dict)\n",
        "training_dict = subset_data_dict (image_dict, training_image_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dict"
      ],
      "metadata": {
        "id": "FmQLo9Bxz1cV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LdSymP2ez3GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e2Ld0wwzEEZ"
      },
      "outputs": [],
      "source": [
        "# Prepare tokenizer\n",
        "tokenizer, vocab_size, max_caption_words = create_tokenizer(training_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1JE4rHzzHgk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare Training Data using a Tensorflow Dataset**"
      ],
      "metadata": {
        "id": "aEovBzyqzBWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have pre-processed images and captions. We go through every training image and its matching captions to prepare the training data. This consists of:\n",
        "\n",
        "Features (X) consisting of the image file paths\n",
        "Targets (y) consisting of the cleaned and tokenized captions\n"
      ],
      "metadata": {
        "id": "iqjasq4qzH4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_prep(data_dict, tokenizer, max_length, vocab_size):\n",
        "  X, y = list(), list()\n",
        "\n",
        "  # For each image and list of captions\n",
        "  for image_name, captions in data_dict.items():\n",
        "    image_name = image_dir + image_name + '.jpg'\n",
        "\n",
        "    # For each caption in the list of captions\n",
        "    for caption in captions:\n",
        "\n",
        "      # Convert the caption words into a list of word indices\n",
        "      word_idxs = tokenizer.texts_to_sequences([caption])[0]\n",
        "\n",
        "      # Pad the input text to the same fixed length\n",
        "      pad_idxs = pad_text(word_idxs, max_length)\n",
        "          \n",
        "      X.append(image_name)\n",
        "      y.append(pad_idxs)\n",
        "  \n",
        "  #return array(X), array(y)\n",
        "  return X, y\n"
      ],
      "metadata": {
        "id": "xWW-ZezBzNyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, train_y = data_prep(training_dict, tokenizer, max_caption_words, vocab_size)"
      ],
      "metadata": {
        "id": "qKfOy0sfzUnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "wphpkbzS1WtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-hdp_481kxl",
        "outputId": "3307c08d-ef68-4a58-964e-8cdb28f8b08b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/Images/1000268201_693b08cb0e.jpg',\n",
              " '/content/Images/1000268201_693b08cb0e.jpg',\n",
              " '/content/Images/1000268201_693b08cb0e.jpg',\n",
              " '/content/Images/1000268201_693b08cb0e.jpg',\n",
              " '/content/Images/1000268201_693b08cb0e.jpg',\n",
              " '/content/Images/1001773457_577c3a7d70.jpg',\n",
              " '/content/Images/1001773457_577c3a7d70.jpg',\n",
              " '/content/Images/1001773457_577c3a7d70.jpg',\n",
              " '/content/Images/1001773457_577c3a7d70.jpg',\n",
              " '/content/Images/1001773457_577c3a7d70.jpg']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#np.array(train_X)"
      ],
      "metadata": {
        "id": "cestIjqI0vmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we use tensorflow Dataset object so that the inputs are fetched and feeded to the model effectively, one batch at a time\n",
        "The data is fetched batch wise so that it doesn’t all have to be in memory at the same time. This allows us to support very large datasets.\n",
        "\n",
        "The dataset loads the pre-processed encoded image vectors that were saved earlier. It uses the image file name to identify the saved file path"
      ],
      "metadata": {
        "id": "xbjcTei7166G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "# Load the numpy files\n",
        "def map_func(img_name, cap):\n",
        "   img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
        "   return img_tensor, cap\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((train_X, train_y))\n",
        "\n",
        "# Use map to load the numpy files in parallel\n",
        "dataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]),num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Shuffle and batch\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "nwZ7xOCL2Qn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image Caption Model with Attention**"
      ],
      "metadata": {
        "id": "RZpX6DsF2nrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, features, hidden):\n",
        "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
        "\n",
        "    # hidden shape == (batch_size, hidden_size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "    # attention_hidden_layer shape == (batch_size, 64, units)\n",
        "    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
        "                                         self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # score shape == (batch_size, 64, 1)\n",
        "    # This gives you an unnormalized score for each image feature.\n",
        "    score = self.V(attention_hidden_layer)\n",
        "\n",
        "    # attention_weights shape == (batch_size, 64, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * features\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "sP4Eu0HF2puS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_Encoder(tf.keras.Model):\n",
        "    # Since you have already extracted the features and dumped it\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        # shape after fc == (batch_size, 64, embedding_dim)\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "dqxgdtRw28d8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units, vocab_size):\n",
        "    super(RNN_Decoder, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "  def call(self, x, features, hidden):\n",
        "    # defining attention as a separate model\n",
        "    context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # shape == (batch_size, max_length, hidden_size)\n",
        "    x = self.fc1(output)\n",
        "\n",
        "    # x shape == (batch_size * max_length, hidden_size)\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size * max_length, vocab)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "  def reset_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))"
      ],
      "metadata": {
        "id": "pdGCO_Ho3Fj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "Sw4Byi4o3oJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_size = vocab_size\n",
        "num_steps = len(train_X) // BATCH_SIZE\n",
        "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
        "# These two variables represent that vector shape\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64\n",
        "\n",
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "metadata": {
        "id": "JJeObhTj3rMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n"
      ],
      "metadata": {
        "id": "CK04BeqP4ELV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_plot = []\n",
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "  loss = 0\n",
        "\n",
        "  # initializing the hidden state for each batch\n",
        "  # because the captions are not related from image to image\n",
        "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "\n",
        "  dec_input = tf.expand_dims([tokenizer.word_index['startseq']] * target.shape[0], 1)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "      features = encoder(img_tensor)\n",
        "\n",
        "      for i in range(1, target.shape[1]):\n",
        "          # passing the features through the decoder\n",
        "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "\n",
        "          loss += loss_function(target[:, i], predictions)\n",
        "\n",
        "          # using teacher forcing\n",
        "          dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "  total_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "  return loss, total_loss\n"
      ],
      "metadata": {
        "id": "HymKUlqP4IPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_epoch = 0\n",
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
        "        batch_loss, t_loss = train_step(img_tensor, target)\n",
        "        total_loss += t_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
        "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
        "    # storing the epoch end loss value to plot later\n",
        "    loss_plot.append(total_loss / num_steps)\n",
        "\n",
        "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
        "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqM8qCTC3MQy",
        "outputId": "60fc3eff-bf0f-49d4-9005-d87501dae96e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.9231\n",
            "Epoch 1 Batch 100 Loss 1.8246\n",
            "Epoch 1 Batch 200 Loss 1.5414\n",
            "Epoch 1 Batch 300 Loss 1.3983\n",
            "Epoch 1 Batch 400 Loss 1.3326\n",
            "Epoch 1 Loss 1.617978\n",
            "Time taken for 1 epoch 132.48 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.3333\n",
            "Epoch 2 Batch 100 Loss 1.2805\n",
            "Epoch 2 Batch 200 Loss 1.3346\n",
            "Epoch 2 Batch 300 Loss 1.2891\n",
            "Epoch 2 Batch 400 Loss 1.2554\n",
            "Epoch 2 Loss 1.252310\n",
            "Time taken for 1 epoch 51.20 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.1185\n",
            "Epoch 3 Batch 100 Loss 1.0844\n",
            "Epoch 3 Batch 200 Loss 1.2127\n",
            "Epoch 3 Batch 300 Loss 1.1400\n",
            "Epoch 3 Batch 400 Loss 1.0753\n",
            "Epoch 3 Loss 1.119503\n",
            "Time taken for 1 epoch 49.29 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.3160\n",
            "Epoch 4 Batch 100 Loss 1.0781\n",
            "Epoch 4 Batch 200 Loss 0.9513\n",
            "Epoch 4 Batch 300 Loss 0.9602\n",
            "Epoch 4 Batch 400 Loss 1.1042\n",
            "Epoch 4 Loss 1.028822\n",
            "Time taken for 1 epoch 49.88 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.9326\n",
            "Epoch 5 Batch 100 Loss 0.9055\n",
            "Epoch 5 Batch 200 Loss 1.0325\n",
            "Epoch 5 Batch 300 Loss 0.8713\n",
            "Epoch 5 Batch 400 Loss 0.8707\n",
            "Epoch 5 Loss 0.955706\n",
            "Time taken for 1 epoch 49.93 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.9808\n",
            "Epoch 6 Batch 100 Loss 0.7695\n",
            "Epoch 6 Batch 200 Loss 0.9798\n",
            "Epoch 6 Batch 300 Loss 0.8394\n",
            "Epoch 6 Batch 400 Loss 0.9124\n",
            "Epoch 6 Loss 0.894358\n",
            "Time taken for 1 epoch 47.79 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.9193\n",
            "Epoch 7 Batch 100 Loss 0.8345\n",
            "Epoch 7 Batch 200 Loss 0.8107\n",
            "Epoch 7 Batch 300 Loss 0.8601\n",
            "Epoch 7 Batch 400 Loss 0.7977\n",
            "Epoch 7 Loss 0.838588\n",
            "Time taken for 1 epoch 49.64 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.7902\n",
            "Epoch 8 Batch 100 Loss 0.7895\n",
            "Epoch 8 Batch 200 Loss 0.9282\n",
            "Epoch 8 Batch 300 Loss 0.7542\n",
            "Epoch 8 Batch 400 Loss 0.7898\n",
            "Epoch 8 Loss 0.788524\n",
            "Time taken for 1 epoch 49.19 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.7765\n",
            "Epoch 9 Batch 100 Loss 0.7683\n",
            "Epoch 9 Batch 200 Loss 0.7210\n",
            "Epoch 9 Batch 300 Loss 0.7714\n",
            "Epoch 9 Batch 400 Loss 0.7554\n",
            "Epoch 9 Loss 0.743428\n",
            "Time taken for 1 epoch 60.82 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.8120\n",
            "Epoch 10 Batch 100 Loss 0.6873\n",
            "Epoch 10 Batch 200 Loss 0.7782\n",
            "Epoch 10 Batch 300 Loss 0.7059\n",
            "Epoch 10 Batch 400 Loss 0.7545\n",
            "Epoch 10 Loss 0.715255\n",
            "Time taken for 1 epoch 49.49 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.8105\n",
            "Epoch 11 Batch 100 Loss 0.6689\n",
            "Epoch 11 Batch 200 Loss 0.6382\n",
            "Epoch 11 Batch 300 Loss 0.6200\n",
            "Epoch 11 Batch 400 Loss 0.6234\n",
            "Epoch 11 Loss 0.673651\n",
            "Time taken for 1 epoch 50.30 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.5621\n",
            "Epoch 12 Batch 100 Loss 0.6755\n",
            "Epoch 12 Batch 200 Loss 0.6322\n",
            "Epoch 12 Batch 300 Loss 0.6124\n",
            "Epoch 12 Batch 400 Loss 0.6302\n",
            "Epoch 12 Loss 0.630807\n",
            "Time taken for 1 epoch 52.73 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.6138\n",
            "Epoch 13 Batch 100 Loss 0.6235\n",
            "Epoch 13 Batch 200 Loss 0.6448\n",
            "Epoch 13 Batch 300 Loss 0.5592\n",
            "Epoch 13 Batch 400 Loss 0.6307\n",
            "Epoch 13 Loss 0.597154\n",
            "Time taken for 1 epoch 49.78 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.6103\n",
            "Epoch 14 Batch 100 Loss 0.5504\n",
            "Epoch 14 Batch 200 Loss 0.5728\n",
            "Epoch 14 Batch 300 Loss 0.5584\n",
            "Epoch 14 Batch 400 Loss 0.4881\n",
            "Epoch 14 Loss 0.566447\n",
            "Time taken for 1 epoch 52.23 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.5234\n",
            "Epoch 15 Batch 100 Loss 0.5294\n",
            "Epoch 15 Batch 200 Loss 0.5443\n",
            "Epoch 15 Batch 300 Loss 0.5796\n",
            "Epoch 15 Batch 400 Loss 0.5836\n",
            "Epoch 15 Loss 0.539051\n",
            "Time taken for 1 epoch 51.31 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.6111\n",
            "Epoch 16 Batch 100 Loss 0.4959\n",
            "Epoch 16 Batch 200 Loss 0.5394\n",
            "Epoch 16 Batch 300 Loss 0.4863\n",
            "Epoch 16 Batch 400 Loss 0.4239\n",
            "Epoch 16 Loss 0.514209\n",
            "Time taken for 1 epoch 50.95 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.5220\n",
            "Epoch 17 Batch 100 Loss 0.4491\n",
            "Epoch 17 Batch 200 Loss 0.5242\n",
            "Epoch 17 Batch 300 Loss 0.4998\n",
            "Epoch 17 Batch 400 Loss 0.5373\n",
            "Epoch 17 Loss 0.489080\n",
            "Time taken for 1 epoch 49.80 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.5222\n",
            "Epoch 18 Batch 100 Loss 0.5238\n",
            "Epoch 18 Batch 200 Loss 0.4718\n",
            "Epoch 18 Batch 300 Loss 0.4750\n",
            "Epoch 18 Batch 400 Loss 0.4225\n",
            "Epoch 18 Loss 0.466400\n",
            "Time taken for 1 epoch 51.00 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.4059\n",
            "Epoch 19 Batch 100 Loss 0.4553\n",
            "Epoch 19 Batch 200 Loss 0.4558\n",
            "Epoch 19 Batch 300 Loss 0.3769\n",
            "Epoch 19 Batch 400 Loss 0.4200\n",
            "Epoch 19 Loss 0.445088\n",
            "Time taken for 1 epoch 51.27 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.4881\n",
            "Epoch 20 Batch 100 Loss 0.4109\n",
            "Epoch 20 Batch 200 Loss 0.3767\n",
            "Epoch 20 Batch 300 Loss 0.4425\n",
            "Epoch 20 Batch 400 Loss 0.4477\n",
            "Epoch 20 Loss 0.426908\n",
            "Time taken for 1 epoch 50.18 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(image, max_length):\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n",
        "                                                 -1,\n",
        "                                                 img_tensor_val.shape[3]))\n",
        "\n",
        "    features = encoder(img_tensor_val)\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['startseq']], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input,\n",
        "                                                         features,\n",
        "                                                         hidden)\n",
        "\n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "        result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "        if tokenizer.index_word[predicted_id] == 'endseq':\n",
        "            return result, attention_plot\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    attention_plot = attention_plot[:len(result), :]\n",
        "    return result, attention_plot\n"
      ],
      "metadata": {
        "id": "ewqH-CAj2T8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XOfUf5AN-aqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_test(test_image_names, image_dict, image_dir, max_caption_words):\n",
        "  # captions on the validation set\n",
        "  rid = np.random.randint(0, len(test_image_names))\n",
        "  image_name = test_image_names[rid]\n",
        "  real_caption = image_dict[image_name]\n",
        "\n",
        "  image_path = image_dir + image_name + '.jpg'\n",
        "  result, attention_plot = evaluate(image_path, max_caption_words)\n",
        "\n",
        "  from IPython.display import Image, display\n",
        "  display(Image(image_path))\n",
        "  print('Real Caption:', real_caption)\n",
        "  print('Prediction Caption:', ' '.join(result[:-1]))\n",
        "  \n"
      ],
      "metadata": {
        "id": "zK27bIQs9Z8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_name_file = \"/content/gdrive/MyDrive/Flicker8k/test_images.txt\"\n",
        "test_image_names = subset_image_name (test_image_name_file)\n",
        "test_image_names = list(test_image_names)\n",
        "image_dir = \"/content/Images/\"\n",
        "check_test(test_image_names, image_dict, image_dir, max_caption_words)"
      ],
      "metadata": {
        "id": "wnBkvWKvc_EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "# Save the optimizer state for encoder\n",
        "with open('/content/gdrive/MyDrive/Saved_Models/Image_Caption_Gen/encoder_optimizer.pkl', 'wb') as f:\n",
        "    pickle.dump(encoder.get_config(), f)\n",
        "\n",
        "# Save the optimizer state for decoder\n",
        "with open('/content/gdrive/MyDrive/Saved_Models/Image_Caption_Gen/decoder_optimizer.pkl', 'wb') as f:\n",
        "    pickle.dump(decoder.get_config(), f)"
      ],
      "metadata": {
        "id": "QBAbld_PeP7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.get_config()\n",
        "decoder.get_config()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTcrgTK4glR7",
        "outputId": "9d2501e7-3f30-448a-ab9b-c39213c4cf96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'embedding_dim': 256, 'units': 512, 'vocab_size': 7076}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the encoder model\n",
        "# encoder.save('/content/gdrive/MyDrive/Saved_Models/Image_Caption_Gen/encoder_model.h5')\n",
        "\n",
        "# # Save the decoder model\n",
        "# decoder.save('/content/gdrive/MyDrive/Saved_Models/Image_Caption_Gen/decoder_model.h5')\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Saved_Models/Image_Caption_Gen/encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(encoder, f)\n",
        "\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Saved_Models/Image_Caption_Gen/decoder.pkl', 'wb') as f:\n",
        "    pickle.dump(decoder, f)\n",
        "\n"
      ],
      "metadata": {
        "id": "GyAoermFg3GA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7kP7hj0zXo8uukeAu097h",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}